def backpropagate(network,input_vector, targets):
    
    hidden_outputs, outputs = feedForward(network,input_vector)
    
    #the output * (1-output) is from the derivative of sigmoid
    output_deltas = [output*(1-output) * (output - target)
                    for output, target in zip(outputs,targets)]
    
    #adjust weights for output layer, one neuron at a time
    for i, output_neuron in enumerate(network[-1]):
        #focus on the ith output layer neuron
        for j, hidden_output in enumerate(hidden_outputs + [1]):
            #adjust the jth weight based on both
            #this neuron's delta and its jth input
            output_neuron[j] -= output_deltas[i] * hidden_output
            
    #back propagate errors to hidden layer
    hidden_deltas = [hidden_output * (1- hidden_output) * 
                    dot(output_deltas, [n[i] for n in network[-1]])
                    for i, hidden_output in enumerate(hidden_outputs)]
    
    #adjust weights for hidden layer, one neuron at a time
    for i, hidden_neuron in enumerate(network[0]):
        for j, input in enumerate(input_vector + [1]):
            hidden_neuron[j] -= hidden_deltas[i] * input
            
zero_digit = [1,1,1,1,1,
              1,0,0,0,1,
              1,0,0,0,1,
              1,0,0,0,1,
              1,1,1,1,1]

one_digit = [0,0,1,0,0,
              0,0,1,0,0,
              0,0,1,0,0,
              0,0,1,0,0,
              0,0,1,0,0]

two_digit = [1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1,
              1,0,0,0,0,
              1,1,1,1,1]

three_digit = [1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1]

four_digit = [1,0,0,0,1,
              1,0,0,0,1,
              1,1,1,1,1,
              0,0,0,0,1,
              0,0,0,0,1]

five_digit = [1,1,1,1,1,
              1,0,0,0,0,
              1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1]

six_digit =  [1,1,1,1,1,
              1,0,0,0,0,
              1,1,1,1,1,
              1,0,0,0,1,
              1,1,1,1,1]

seven_digit = [1,1,1,1,1,
              0,0,0,0,1,
              0,0,0,0,1,
              0,0,0,0,1,
              0,0,0,0,1]

eight_digit = [1,1,1,1,1,
              1,0,0,0,1,
              1,1,1,1,1,
              1,0,0,0,1,
              1,1,1,1,1]

nine_digit = [1,1,1,1,1,
              1,0,0,0,1,
              1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1]

inputs = [zero_digit,one_digit,two_digit,three_digit,four_digit,
          five_digit,six_digit,seven_digit, eight_digit,nine_digit]
          
targets = [[1 if i == j else 0 for i in range(10)]
          for j in range(10)]

random.seed(0)  #to get repeatable results
input_size = 25 #each input is a vector of length 25
num_hidden = 5 #we'll have 5 neurons in the hidden layer
output_size = 10 #we'll need 10 outputs for each input

#each hidden neuron has one weight per input, plus a bias weight
hidden_layer = [[random.random() for _ in range(input_size + 1)]
               for _ in range(num_hidden)]

#each output neuron has one weight per hidden neuron, plus a bias weight
output_layer = [[random.random() for _ in range(num_hidden + 1)]
               for _ in range(output_size)]

#the network starts out with random weights
network = [hidden_layer, output_layer]

#10,000 iterations seems enough to converge
for m in range(10000):
    for input_vector, target_vector in zip(inputs, targets):
        backpropagate(network,input_vector, target_vector)
        
def predict(input):
    return feedForward(network,input)[-1]
    
predict([0,0,1,1,0,
         0,0,0,0,1,
         0,0,1,1,1,
         0,0,0,0,1,
         0,0,1,1,0])
        
