'''Neural Networks
from Grus, Data Science from Scratch, Ch 18
May 21, 2017'''

from math import exp
from numpy import dot
import random

def stepFunction(x):
    return 1 if x >= 0 else 0

def sigmoid(t):
    return 1/(1+exp(-t))

def dot2(a,b):
    '''dot product of 2x2 matrices'''
    return a[0]*b[0]+a[1]*b[1]

def perceptronOutput(weights,bias,x):
    '''returns 2 if the perceptron fires, 0 if not'''
    calculation = dot(weights,x) + bias
    return stepFunction(calculation)

def gates():
    inputs = [[0,0],[0,1],[1,0],[1,1]]

    for i in inputs:
        print(i,perceptronOutput([2,2],-3,i))

def neuronOutput(weights,inputs):
    return round(sigmoid(dot(weights, inputs)),3)

def feedForward(neural_network, input_vector):
    '''takes in a neural network
    (represented as a list of lists of lists of weights)
    and returns the output from forward-propagating the input'''

    outputs = []

    #process one layer at a time
    for layer in neural_network:
        input_with_bias = input_vector + [1]            #add a bias input
        output = [neuronOutput(neuron,input_with_bias) #compute the output
                  for neuron in layer]                  #for each neuron
        outputs.append(output)

        #then the input to the next layer is the output of this one
        input_vector = output
    #print("outputs: ",outputs)
    return outputs

xor_network = [#hidden layer
                [[20,20,-30], #AND neuron
                 [20,20,-10]], #OR neuron
                #output layer
                [[-60,60,-30]]] #'2nd input but not 1st input' neuron

#Uncomment for XOR gate:
'''for x in [0,1]:
    for y in [0,1]:
        #feed forward produces the outputs of every neuron
        #feed forward[-1] is the outputs of the output-layer neurons
        print(x,y,feedForward(xor_network,[x,y])[-1])'''


def backpropagate(network,input_vector, targets):
    
    hidden_outputs, outputs = feedForward(network,input_vector)
    
    #the output * (1-output) is from the derivative of sigmoid
    output_deltas = [output*(1-output) * (output - target)
                    for output, target in zip(outputs,targets)]
    
    #adjust weights for output layer, one neuron at a time
    for i, output_neuron in enumerate(network[-1]):
        #focus on the ith output layer neuron
        for j, hidden_output in enumerate(hidden_outputs + [1]):
            #adjust the jth weight based on both
            #this neuron's delta and its jth input
            output_neuron[j] -= output_deltas[i] * hidden_output
            
    #back propagate errors to hidden layer
    hidden_deltas = [hidden_output * (1- hidden_output) * 
                    dot(output_deltas, [n[i] for n in network[-1]])
                    for i, hidden_output in enumerate(hidden_outputs)]
    
    #adjust weights for hidden layer, one neuron at a time
    for i, hidden_neuron in enumerate(network[0]):
        for j, input in enumerate(input_vector + [1]):
            hidden_neuron[j] -= hidden_deltas[i] * input
            
zero_digit = [1,1,1,1,1,
              1,0,0,0,1,
              1,0,0,0,1,
              1,0,0,0,1,
              1,1,1,1,1]

one_digit = [0,0,1,0,0,
              0,0,1,0,0,
              0,0,1,0,0,
              0,0,1,0,0,
              0,0,1,0,0]

two_digit = [1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1,
              1,0,0,0,0,
              1,1,1,1,1]

three_digit = [1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1]

four_digit = [1,0,0,0,1,
              1,0,0,0,1,
              1,1,1,1,1,
              0,0,0,0,1,
              0,0,0,0,1]

five_digit = [1,1,1,1,1,
              1,0,0,0,0,
              1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1]

six_digit =  [1,1,1,1,1,
              1,0,0,0,0,
              1,1,1,1,1,
              1,0,0,0,1,
              1,1,1,1,1]

seven_digit = [1,1,1,1,1,
              0,0,0,0,1,
              0,0,0,0,1,
              0,0,0,0,1,
              0,0,0,0,1]

eight_digit = [1,1,1,1,1,
              1,0,0,0,1,
              1,1,1,1,1,
              1,0,0,0,1,
              1,1,1,1,1]

nine_digit = [1,1,1,1,1,
              1,0,0,0,1,
              1,1,1,1,1,
              0,0,0,0,1,
              1,1,1,1,1]

inputs = [zero_digit,one_digit,two_digit,three_digit,four_digit,
          five_digit,six_digit,seven_digit, eight_digit,nine_digit]
          
targets = [[1 if i == j else 0 for i in range(10)]
          for j in range(10)]

random.seed(0)  #to get repeatable results
input_size = 25 #each input is a vector of length 25
num_hidden = 5 #we'll have 5 neurons in the hidden layer
output_size = 10 #we'll need 10 outputs for each input

#each hidden neuron has one weight per input, plus a bias weight
hidden_layer = [[random.random() for _ in range(input_size + 1)]
               for _ in range(num_hidden)]

#each output neuron has one weight per hidden neuron, plus a bias weight
output_layer = [[random.random() for _ in range(num_hidden + 1)]
               for _ in range(output_size)]

#the network starts out with random weights
network = [hidden_layer, output_layer]

#10,000 iterations seems enough to converge
for m in range(10000):
    for input_vector, target_vector in zip(inputs, targets):
        backpropagate(network,input_vector, target_vector)
        
def predict(input):
    return feedForward(network,input)[-1]
    
predict([0,0,1,1,0,
         0,0,0,0,1,
         0,0,1,1,1,
         0,0,0,0,1,
         0,0,1,1,0])
        
